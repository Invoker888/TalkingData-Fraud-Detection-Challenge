{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning... Mon May 14 08:59:57 2018\n",
      "Reading dataset from disk file...\n",
      "Loading data time: 911.7561986446381\n",
      "predictors ['app', 'device', 'os', 'channel', 'hour', 'nextClick', 'ip_tcount', 'ip_app_count', 'ip_app_os_count', 'ip_app_channel_mean_hour', 'ip_app_os_var', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X8', 'A0', 'A2', 'nextClick2', 'nextClick4', 'nextClick5', 'nextClick6', 'B1', 'B2', 'B3', 'B4', 'C0', 'C1']\n",
      "Generate 9th:4,5,9,10,13,14 +8h validation set...\n",
      "Training beings...\n",
      "load train_df into lgb.Dataset...\n",
      "load valid_df into lgb.Dataset...\n",
      "Start doing lgb.train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "D:\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
      "  warnings.warn('categorical_feature in param dict is overrided.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 80 rounds.\n",
      "[10]\tvalid's auc: 0.962036\n",
      "[20]\tvalid's auc: 0.963527\n",
      "[30]\tvalid's auc: 0.967438\n",
      "[40]\tvalid's auc: 0.96981\n",
      "[50]\tvalid's auc: 0.971229\n",
      "[60]\tvalid's auc: 0.972523\n",
      "[70]\tvalid's auc: 0.973804\n",
      "[80]\tvalid's auc: 0.975062\n",
      "[90]\tvalid's auc: 0.976066\n",
      "[100]\tvalid's auc: 0.976832\n",
      "[110]\tvalid's auc: 0.977648\n",
      "[120]\tvalid's auc: 0.978217\n",
      "[130]\tvalid's auc: 0.978684\n",
      "[140]\tvalid's auc: 0.979059\n",
      "[150]\tvalid's auc: 0.979396\n",
      "[160]\tvalid's auc: 0.979637\n",
      "[170]\tvalid's auc: 0.979837\n",
      "[180]\tvalid's auc: 0.980081\n",
      "[190]\tvalid's auc: 0.980301\n",
      "[200]\tvalid's auc: 0.98043\n",
      "[210]\tvalid's auc: 0.980625\n",
      "[220]\tvalid's auc: 0.980786\n",
      "[230]\tvalid's auc: 0.980916\n",
      "[240]\tvalid's auc: 0.980988\n",
      "[250]\tvalid's auc: 0.981077\n",
      "[260]\tvalid's auc: 0.981179\n",
      "[270]\tvalid's auc: 0.981265\n",
      "[280]\tvalid's auc: 0.981339\n",
      "[290]\tvalid's auc: 0.98143\n",
      "[300]\tvalid's auc: 0.981513\n",
      "[310]\tvalid's auc: 0.981582\n",
      "[320]\tvalid's auc: 0.98164\n",
      "[330]\tvalid's auc: 0.9817\n",
      "[340]\tvalid's auc: 0.981761\n",
      "[350]\tvalid's auc: 0.981816\n",
      "[360]\tvalid's auc: 0.981861\n",
      "[370]\tvalid's auc: 0.981925\n",
      "[380]\tvalid's auc: 0.981977\n",
      "[390]\tvalid's auc: 0.982012\n",
      "[400]\tvalid's auc: 0.982067\n",
      "[410]\tvalid's auc: 0.98212\n",
      "[420]\tvalid's auc: 0.982154\n",
      "[430]\tvalid's auc: 0.982192\n",
      "[440]\tvalid's auc: 0.982241\n",
      "[450]\tvalid's auc: 0.982269\n",
      "[460]\tvalid's auc: 0.982311\n",
      "[470]\tvalid's auc: 0.982346\n",
      "[480]\tvalid's auc: 0.982378\n",
      "[490]\tvalid's auc: 0.982407\n",
      "[500]\tvalid's auc: 0.982447\n",
      "[510]\tvalid's auc: 0.982475\n",
      "[520]\tvalid's auc: 0.982507\n",
      "[530]\tvalid's auc: 0.982525\n",
      "[540]\tvalid's auc: 0.982543\n",
      "[550]\tvalid's auc: 0.982567\n",
      "[560]\tvalid's auc: 0.982589\n",
      "[570]\tvalid's auc: 0.982622\n",
      "[580]\tvalid's auc: 0.982651\n",
      "[590]\tvalid's auc: 0.98268\n",
      "[600]\tvalid's auc: 0.982696\n",
      "[610]\tvalid's auc: 0.98272\n",
      "[620]\tvalid's auc: 0.982748\n",
      "[630]\tvalid's auc: 0.982775\n",
      "[640]\tvalid's auc: 0.982788\n",
      "[650]\tvalid's auc: 0.982798\n",
      "[660]\tvalid's auc: 0.982819\n",
      "[670]\tvalid's auc: 0.982832\n",
      "[680]\tvalid's auc: 0.982847\n",
      "[690]\tvalid's auc: 0.982864\n",
      "[700]\tvalid's auc: 0.982889\n",
      "[710]\tvalid's auc: 0.982903\n",
      "[720]\tvalid's auc: 0.982923\n",
      "[730]\tvalid's auc: 0.982936\n",
      "[740]\tvalid's auc: 0.982959\n",
      "[750]\tvalid's auc: 0.982984\n",
      "[760]\tvalid's auc: 0.982997\n",
      "[770]\tvalid's auc: 0.983015\n",
      "[780]\tvalid's auc: 0.983028\n",
      "[790]\tvalid's auc: 0.983044\n",
      "[800]\tvalid's auc: 0.98305\n",
      "[810]\tvalid's auc: 0.983058\n",
      "[820]\tvalid's auc: 0.983065\n",
      "[830]\tvalid's auc: 0.983078\n",
      "[840]\tvalid's auc: 0.983088\n",
      "[850]\tvalid's auc: 0.983097\n",
      "[860]\tvalid's auc: 0.983111\n",
      "[870]\tvalid's auc: 0.983123\n",
      "[880]\tvalid's auc: 0.983134\n",
      "[890]\tvalid's auc: 0.983146\n",
      "[900]\tvalid's auc: 0.983152\n",
      "[910]\tvalid's auc: 0.983157\n",
      "[920]\tvalid's auc: 0.983168\n",
      "[930]\tvalid's auc: 0.983175\n",
      "[940]\tvalid's auc: 0.983186\n",
      "[950]\tvalid's auc: 0.983192\n",
      "[960]\tvalid's auc: 0.983201\n",
      "[970]\tvalid's auc: 0.98321\n",
      "[980]\tvalid's auc: 0.983213\n",
      "[990]\tvalid's auc: 0.983219\n",
      "[1000]\tvalid's auc: 0.983225\n",
      "[1010]\tvalid's auc: 0.983235\n",
      "[1020]\tvalid's auc: 0.983246\n",
      "[1030]\tvalid's auc: 0.983251\n",
      "[1040]\tvalid's auc: 0.983259\n",
      "[1050]\tvalid's auc: 0.983263\n",
      "[1060]\tvalid's auc: 0.983268\n",
      "[1070]\tvalid's auc: 0.983274\n",
      "[1080]\tvalid's auc: 0.983283\n",
      "[1090]\tvalid's auc: 0.983287\n",
      "[1100]\tvalid's auc: 0.98329\n",
      "[1110]\tvalid's auc: 0.983295\n",
      "[1120]\tvalid's auc: 0.983298\n",
      "[1130]\tvalid's auc: 0.983308\n",
      "[1140]\tvalid's auc: 0.983312\n",
      "[1150]\tvalid's auc: 0.983312\n",
      "[1160]\tvalid's auc: 0.983312\n",
      "[1170]\tvalid's auc: 0.98332\n",
      "[1180]\tvalid's auc: 0.983324\n",
      "[1190]\tvalid's auc: 0.983324\n",
      "[1200]\tvalid's auc: 0.983327\n",
      "[1210]\tvalid's auc: 0.983332\n",
      "[1220]\tvalid's auc: 0.983337\n",
      "[1230]\tvalid's auc: 0.983339\n",
      "[1240]\tvalid's auc: 0.98334\n",
      "[1250]\tvalid's auc: 0.983346\n",
      "[1260]\tvalid's auc: 0.983355\n",
      "[1270]\tvalid's auc: 0.983356\n",
      "[1280]\tvalid's auc: 0.98336\n",
      "[1290]\tvalid's auc: 0.983366\n",
      "[1300]\tvalid's auc: 0.983373\n",
      "[1310]\tvalid's auc: 0.983376\n",
      "[1320]\tvalid's auc: 0.983379\n",
      "[1330]\tvalid's auc: 0.983379\n",
      "[1340]\tvalid's auc: 0.983382\n",
      "[1350]\tvalid's auc: 0.983384\n",
      "[1360]\tvalid's auc: 0.983384\n",
      "[1370]\tvalid's auc: 0.983386\n",
      "[1380]\tvalid's auc: 0.98339\n",
      "[1390]\tvalid's auc: 0.983393\n",
      "[1400]\tvalid's auc: 0.983395\n",
      "[1410]\tvalid's auc: 0.983397\n",
      "[1420]\tvalid's auc: 0.9834\n",
      "[1430]\tvalid's auc: 0.9834\n",
      "[1440]\tvalid's auc: 0.9834\n",
      "[1450]\tvalid's auc: 0.983403\n",
      "[1460]\tvalid's auc: 0.983407\n",
      "[1470]\tvalid's auc: 0.983407\n",
      "[1480]\tvalid's auc: 0.983407\n",
      "[1490]\tvalid's auc: 0.983408\n",
      "[1500]\tvalid's auc: 0.983414\n",
      "[1510]\tvalid's auc: 0.983418\n",
      "[1520]\tvalid's auc: 0.98342\n",
      "[1530]\tvalid's auc: 0.983424\n",
      "[1540]\tvalid's auc: 0.983428\n",
      "[1550]\tvalid's auc: 0.983426\n",
      "[1560]\tvalid's auc: 0.983427\n",
      "[1570]\tvalid's auc: 0.98343\n",
      "[1580]\tvalid's auc: 0.983433\n",
      "[1590]\tvalid's auc: 0.983436\n",
      "[1600]\tvalid's auc: 0.983437\n",
      "[1610]\tvalid's auc: 0.983441\n",
      "[1620]\tvalid's auc: 0.983442\n",
      "[1630]\tvalid's auc: 0.983443\n",
      "[1640]\tvalid's auc: 0.983444\n",
      "[1650]\tvalid's auc: 0.983444\n",
      "[1660]\tvalid's auc: 0.983445\n",
      "[1670]\tvalid's auc: 0.983448\n",
      "[1680]\tvalid's auc: 0.98345\n",
      "[1690]\tvalid's auc: 0.983452\n",
      "[1700]\tvalid's auc: 0.983457\n",
      "[1710]\tvalid's auc: 0.983456\n",
      "[1720]\tvalid's auc: 0.983462\n",
      "[1730]\tvalid's auc: 0.983465\n",
      "[1740]\tvalid's auc: 0.983468\n",
      "[1750]\tvalid's auc: 0.983469\n",
      "[1760]\tvalid's auc: 0.983467\n",
      "[1770]\tvalid's auc: 0.98347\n",
      "[1780]\tvalid's auc: 0.983471\n",
      "[1790]\tvalid's auc: 0.983472\n",
      "[1800]\tvalid's auc: 0.983471\n",
      "[1810]\tvalid's auc: 0.983473\n",
      "[1820]\tvalid's auc: 0.983474\n",
      "[1830]\tvalid's auc: 0.983477\n",
      "[1840]\tvalid's auc: 0.983478\n",
      "[1850]\tvalid's auc: 0.983481\n",
      "[1860]\tvalid's auc: 0.983478\n",
      "[1870]\tvalid's auc: 0.98348\n",
      "[1880]\tvalid's auc: 0.983479\n",
      "[1890]\tvalid's auc: 0.983479\n",
      "[1900]\tvalid's auc: 0.983478\n",
      "[1910]\tvalid's auc: 0.98348\n",
      "[1920]\tvalid's auc: 0.983481\n",
      "[1930]\tvalid's auc: 0.983482\n",
      "[1940]\tvalid's auc: 0.983481\n",
      "[1950]\tvalid's auc: 0.983482\n",
      "[1960]\tvalid's auc: 0.983484\n",
      "[1970]\tvalid's auc: 0.983486\n",
      "[1980]\tvalid's auc: 0.983489\n",
      "[1990]\tvalid's auc: 0.983491\n",
      "[2000]\tvalid's auc: 0.983494\n",
      "[2010]\tvalid's auc: 0.983495\n",
      "[2020]\tvalid's auc: 0.983497\n",
      "[2030]\tvalid's auc: 0.983497\n",
      "[2040]\tvalid's auc: 0.983494\n",
      "[2050]\tvalid's auc: 0.983494\n",
      "[2060]\tvalid's auc: 0.983496\n",
      "[2070]\tvalid's auc: 0.983496\n",
      "[2080]\tvalid's auc: 0.983496\n",
      "[2090]\tvalid's auc: 0.983496\n",
      "Early stopping, best iteration is:\n",
      "[2018]\tvalid's auc: 0.983498\n",
      "\n",
      "Model Report\n",
      "bst1.best_iteration:  2018\n",
      "auc: 0.9834975392343229\n",
      "\n",
      "[20414.986735343933]: model training time\n",
      "Predicting...\n",
      "Writing result...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1649\u001b[0m                 \u001b[0mwriter_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mUnicodeWriter\u001b[1;34m(f, dialect, encoding, **kwds)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdialect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 1 must have a \"write\" method",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7f205d720719>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0msub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/test.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint32'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'click_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'click_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'click_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'is_attributed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'C:\\Users\\Administrator\\Desktop\\submit_V%d.csv'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileno\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%.9f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;31m#         test_df[['click_id','is_attributed']].to_csv('./submit_V%d.csv'%(fileno),index=False,float_format='%.9f')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Save feature importances...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1522\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1524\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1656\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Original kernel:non-blending lightGBM model LB: 0.977:https://www.kaggle.com/bk0000/non-blending-lightgbm-model-lb-0-977?scriptVersionId=3224614\n",
    "V0 Modified by Andy:Kaggle-runnable version of Baris Kanber's LightGBM:https://www.kaggle.com/aharless/kaggle-runnable-version-of-baris-kanber-s-lightgbm/comments\n",
    "A non-blending lightGBM model that incorporates portions and ideas from various public kernels.\n",
    "\n",
    "经验教训：\n",
    "1.70%的时间在做特征工程，使用lag feature滞后特征。\n",
    "2.找队友，尽量使用云而非自己电脑的windows。\n",
    "3.最后时间一定要随时刷新kernels。\n",
    "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56283\n",
    "https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56262\n",
    "\n",
    "Modified by Bai:-----------------------------------------------------------------------------------------------------\n",
    "References:\n",
    "- Python学习笔记——可变对象和不可变对象:https://blog.csdn.net/taohuaxinmu123/article/details/39008281\n",
    "- python id()函数, id()函数用于获取对象的内存地址。:https://www.cnblogs.com/dplearning/p/5998112.html\n",
    "- def concatenate_block_managers(Source Code):https://github.com/pandas-dev/pandas/blob/v0.22.0/pandas/core/internals.py\n",
    "* del:Reference count-1, gc.collect():manully garbage clean\n",
    "- Frequently using small parameters have large reference count:https://segmentfault.com/q/1010000000509607\n",
    "- pandas的4种引用与3种复制，是否copy只取决于采用了切片还是花式索引。：https://blog.csdn.net/qtlyx/article/details/70500145\n",
    "- Python内存池管理与缓冲池设计:https://blog.csdn.net/zhzhl202/article/details/7547445\n",
    "- python Pandas DataFrame copy(deep=False) vs copy(deep=True) vs '=':https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs#\n",
    "* Deep copy creates new id's of every object it contains while normal copy only copies the elements from the parent and creates a new id for a variable to which it is copied to.\n",
    "- 机器不学习：一文看懂机器学习时代神器—LightGBM:http://www.360doc.com/content/17/1231/23/40769523_718019029.shtml\n",
    "- 比XGBOOST更快--LightGBM介绍:https://www.jianshu.com/p/48e82dbb142b\n",
    "* xgboost is a little bit more accurate than lightgbm, but use 8*RAM than lightgbm\n",
    "- 机器学习中，有哪些特征选择的工程方法？：https://www.zhihu.com/question/28641663/answer/107680749\n",
    "* Feature selection exhaustivity is O(2^n), usally use greedy method(forward,backwrad) to find the second-best solution\n",
    "- Pearson相关系数是用来衡量两个数据集合是否在一条线上面，它用来衡量定距变量间的线性关系。相关系数是研究变量之间线性相关程度的量。https://baike.baidu.com/item/Pearson%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/6243913\n",
    "- pandas.DataFrame.astype:http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html\n",
    "* 使用单独的ip作为特征没有意义,与其他特征结合使用。\n",
    "* Order of features do affect the performance of LGB.\n",
    "- DataFrame过滤数据时出现\"The truth value of a Series is ambiguous\"错误及其解决办法:https://blog.csdn.net/l460133921/article/details/80101249\n",
    "- Pandas速查手册中文版:https://blog.csdn.net/qq_33399185/article/details/60872853\n",
    "- https://media.readthedocs.org/pdf/lightgbm/latest/lightgbm.pdf\n",
    "* The experiment on Expo data shows about 8x speed-up compared with one-hot coding.\n",
    "* LightGBM offers good accuracy when using native categorical features instead of one-hot coding.\n",
    "- fltsm特征选择神经网络，自带特征变换与选择功能，gcforest\n",
    "- https://github.com/Microsoft/LightGBM/issues/695\n",
    "- https://sites.google.com/view/lauraepp/parameters\n",
    "\n",
    "根本目标：在test_df（10th:4,5,9,10,13,14 +8h）的得分尽量高。\n",
    "假设(已验证)：这四天中每一天的总体分布相似，故将所有数据合在一起生成新的特征逼近总体分布。\n",
    "重要：避免过拟合LB，相信本地valid score,与测试集有些许不同\n",
    "\n",
    "Possible improvement:\n",
    "* Use lightgbm continue training to train full dataset?'init_model'：直接使用所有数据\n",
    "* next_click?\n",
    "* combine the advantages of former kernels?\n",
    "- tune the parameters?\n",
    "* consider using blending to csv?: Overfit the LB?\n",
    "- 【use base model + regressor to blend?】\n",
    "* Add new features on entire dataset one by one?\n",
    "- feature selection\n",
    "* For my own validation (applying full predictions from script, shifted back by 1 day, to test set analogue)\n",
    "- learning rate epsiolon decay\n",
    "* convert dataset 'float32' to 'float64'\n",
    "* convert 'hour' to 3 categories\n",
    "* Add test_supplyment to generate the features：符合假设，正在使用\n",
    "* 99.75:0.25, so scale_pos_weight to 150-250?\n",
    "* 4.26之前历史记录\n",
    "* find more paper anout how we can define fraud click manully\n",
    "* Use PCA to generate the feature.\n",
    "* Add new features!!!!!\n",
    "- 【Use full train.csv to train with validation最后再尝试。】\n",
    "* consider use only 9th:4,5,9,10,13,14 +8h to validate, all data train: overfitting\n",
    "* delete 9th 4h validate??\n",
    "* Fix 18 testdata mapping click_id\n",
    "* skew and var\n",
    "* Cumcount features becomes bigger and bigger, I tried do cumcount group by day and got a better score\n",
    "\n",
    "V0: * Use functions to optimize the coding structure, useful to debug\n",
    "    * Add many new group by features and 'nextClick'\n",
    "    3372.4 seconds,[127]\ttrain's auc: 0.987305\tvalid's auc: 0.972478, LB:0.9761, nchunk=25000000\n",
    "V1: - Add all 'merge' function 'copy=False'，USEFUL for at least computational speed and prevent reach the RAM limit\n",
    "    Ran 2923 seconds, [129]\ttrain's auc: 0.987555\tvalid's auc: 0.972648, LB:0.9764\n",
    "V2: - Add all 'astype' function 'copy=False'\n",
    "    * Use default seed: 'data_random_seed','bagging_seed','feature_fraction_seed':1,2,3\n",
    "    - Add lgb.plot_importance() for both 'split' and 'gain'\n",
    "    - Reorganize the lgb training function\n",
    "    * 2'nextClick' features contains large randomness but EXTREMELY useful\n",
    "    * delete 2'nextClick' features, result same to the fork, [231]\ttrain's auc: 0.984031\tvalid's auc: 0.96488, LB:0.9661\n",
    "    * then delete lowest score 'ip_app_channel_mean_hour' feature, [124]train's auc: 0.981878\tvalid's auc: 0.965349, LB:0.9662\n",
    "V3: * 'nthread':for the best speed, set this to the number of real CPU cores, not the number of threads:http://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "    - delete all the gc.collect() in the feature function\n",
    "    * Different 'nthread' lead to different result. It is fixed to 4 after test.\n",
    "    * Same result with along same code can have large gap in time, maybe due to the server disk, so time IS NOT fixed\n",
    "    - Train on local computer, using GPU\n",
    "    - Using new method to generate feature 'nextClick' rather than hash\n",
    "    kaggle cpu:[151]\ttrain's auc: 0.988041\tvalid's auc: 0.973151,SAME to the fork, LB:0.9770, 3425.9s\n",
    "    local cpu:[64]\ttrain's auc: 0.985661\tvalid's auc: 0.972455,SAME to the fork, LB:0.9761, 1625s\n",
    "    local gpu:[105]\ttrain's auc: 0.987103\tvalid's auc: 0.97292，2235s\n",
    "    local gpu double precesion:[64]\ttrain's auc: 0.985661\tvalid's auc: 0.972455, 1857s, SAME to local cpu\n",
    "V4: - change nchunk from 25000000 to 75000000\n",
    "    CPU:[326]\ttrain's auc: 0.985568\tvalid's auc: 0.990381，LB:0.9794, use 30G swap, almost 5hour\n",
    "    - change nchunk from 75000000 to 140000000\n",
    "    - delete 'day'.'day'只是用来切块每一天的某小时(区分不同天)，本身不用作分类\n",
    "    - delete 'ip_tchan_count','ip_app_channel_mean_hour','ip_app_os_var','X7' temporarily for speed\n",
    "    - [162]train's auc: 0.984942\tvalid's auc: 0.99025, 19709s, LB:0.9791\n",
    "V5: - add 'ip_tchan_count','ip_app_channel_mean_hour','ip_app_os_var','X7' back(IMPORTANT)\n",
    "    - change nchunk from 140000000 to 150000000\n",
    "    [327]\ttrain's auc: 0.985816\tvalid's auc: 0.990797\n",
    "    - add 'day' for test\n",
    "    - early_stopping_rounds=30 to 50\n",
    "    [650]\ttrain's auc: 0.986541\tvalid's auc: 0.99073, 16G RAM+62G swap, 28562s, LB:0.9796\n",
    "V6: - delete 'day'\n",
    "    - change nchunk from 150000000 to all\n",
    "    - convert train/valid/test to 'float32' before beginning training to prevent memory spike\n",
    "    [309]\ttrain's auc: 0.985642\tvalid's auc: 0.990719, 16G RAM+40G swap, 23376s, LB:0.9798\n",
    "    [15766.776413679123]: model training time\n",
    "V7: * Use GPU(GPU0):LightGBMError: b'Invalid Buffer Size', no GPU from now on\n",
    "    - Add all time 8hour to combine three complete days\n",
    "    * 0-482 on 6th after +8h:ingnore, 184903443-184903889 on 10th after +8h\n",
    "    - Save train/valid/test csv to disk for quickly reading\n",
    "    - change val_df from 2.5M to last 40M(40000000) because test_df begins at 4h to 15h too(influnce early stop)\n",
    "    * valid should only include 9th(4,5,9,10,13,14 +8h), train use only 7th, 8th\n",
    "    [525]\ttrain's auc: 0.986902\tvalid's auc: 0.983006, 23155s, LB:0.9795\n",
    "    [21972.033691883087]: model training time\n",
    "V8: - Use 9th(4,5,9,10,13,14 +8h) as validation, simply drop other hours on 9th NOT use to train\n",
    "    - csv take too much space, use pickle instead\n",
    "    - delete xgtrain in valid_sets for training speed\n",
    "    - add new feature: 'A0', 'A1', 'A2'\n",
    "    [630]\tvalid's auc: 【0.982519】, LB:0.9792, 122070801 training data, 20895641 validation data\n",
    "V9: - Use 9th(4,5,9,10,13,14 +8h) as validation, other hours use for training\n",
    "    - 'uint32' to 'float32' lead to float rounding, fix it for 'click_id'\n",
    "    [544]\tvalid's auc: 【0.983071】,[5830.981881141663]: model training time\n",
    "    164008249 training data,20895641 valid, LB:0.9796\n",
    "V10: * 'scale_pos_weight': 200 to 400, LB:0.9793\n",
    "V11: - Add test_supplyment to generate features together\n",
    "     * 'click_id'会乱序，应使用原文件的'click_id'而非index\n",
    "     [512]\tvalid's auc: 【0.98313】, Preparing data time: 17311, LB:0.8159 -> 【0.9810】\n",
    "     * 'scale_pos_weight':300, [507]\tvalid's auc: 0.983033, [5520.847676038742]: model training time\n",
    "     * learning_rate 0.2 to 0.1\n",
    "     [986]\tvalid's auc: 0.983183, [9227.210345506668]: model training time\n",
    "     * 'scale_pos_weight':250, [706]\tvalid's auc: 0.983098\n",
    "     - Fix 'click_id' mapping\n",
    "V12: * learning_rate 0.2 to 0.08\n",
    "     [1593]\tvalid's auc: 【0.98322】, 【LB:0.9811】, 72rd\n",
    "V13: - delete 'X7'双倒数第一\n",
    "     - set learning_rate to 0.2 for speed temporarily\n",
    "     [788]\tvalid's auc: 0.983164, LB:0.9810\n",
    "V14: - add 4 nextClick features\n",
    "     [481]\tvalid's auc: 0.983143，LB：0.9809\n",
    "V15: - delete 'nextClick3'\n",
    "     [512]\tvalid's auc: 【0.98322, LB:0.9811】\n",
    "V16: - add 5 prevClick features\n",
    "     [803]\tvalid's auc: 0.983308,内存已提交76G,11221s, LB:0.9807\n",
    "V17: - delete 'prevClick'\n",
    "     [394]\tvalid's auc: 0.983209, LB:0.9807\n",
    "V18: - delete 'prevClick3','prevClick5'\n",
    "     [423]\tvalid's auc: 0.983272, LB:0.9807\n",
    "V19: - delete 'prevClick2'\n",
    "     [404]\tvalid's auc: 0.983068，LB:0.9809\n",
    "V20: - delete all 'prevClick' useless,相似的特征加入会削弱其他特征的影响\n",
    "     * delete 'A1',add back\n",
    "     [550]\tvalid's auc: 0.98312\n",
    "V21: - add nextClick6,B0-B4, LB:0.9809\n",
    "V22: - add 'C0','C1'\n",
    "     - delete 'X6'双倒一，'ip_tchan_count'\n",
    "     [456]\tvalid's auc: 0.983322, LB:0.9810\n",
    "V23: - delete 'B0','A1','ip_app_channel_var_day'\n",
    "     [508]\tvalid's auc: 【【0.983372, LB:0.9812】】\n",
    "V24: - set learning_rate from 0.2 to 0.05, num_boost_round=3000,early_stopping_rounds=80\n",
    "     - transfer Do to main function\n",
    "     [2018]\tvalid's auc: 【0.983498, LB:0.9813】\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def do_count( df, group_cols, agg_name, agg_type='uint32', show_max=False):\n",
    "    print( \"do_count to \", group_cols , '...' )\n",
    "    gp = df[group_cols].groupby(group_cols).size().rename(agg_name).to_frame().reset_index()\n",
    "#     if debug:\n",
    "#         print(df[group_cols])\n",
    "# #         print(df[group_cols][group_cols]) # same to the former\n",
    "#         print(df[group_cols][group_cols].groupby(group_cols).size())\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max())\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "\n",
    "def do_countuniq( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False):\n",
    "    print( \"Counting unqiue \", counted, \" by \", group_cols , '...' )\n",
    "    # print('the Id of train_df while function before merge: ',id(df)) # the same with train_df\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].nunique().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    # print('the Id of train_df while function after merge: ',id(df)) # id changes\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "    \n",
    "def do_cumcount( df, group_cols, counted, agg_name, agg_type='uint32', show_max=False):\n",
    "    print( \"Cumulative count by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].cumcount() # 累加\n",
    "    df[agg_name]=gp.values\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "\n",
    "def do_mean( df, group_cols, counted, agg_name, agg_type='float32', show_max=False):\n",
    "    print( \"Calculating mean of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].mean().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "\n",
    "def do_var( df, group_cols, counted, agg_name, agg_type='float32', show_max=False):\n",
    "    print( \"Calculating variance of \", counted, \" by \", group_cols , '...' )\n",
    "    gp = df[group_cols+[counted]].groupby(group_cols)[counted].var().reset_index().rename(columns={counted:agg_name})\n",
    "    df = df.merge(gp, on=group_cols, how='left', copy=False)\n",
    "    del gp\n",
    "    if show_max:\n",
    "        print( agg_name + \" max value = \", df[agg_name].max() )\n",
    "    df[agg_name] = df[agg_name].astype(agg_type,copy=False)\n",
    "    return( df )\n",
    "\n",
    "def lgb_modelfit_nocv(dtrain, dvalid, predictors, target='target', feval=None, early_stopping_rounds=30, num_boost_round=3000, verbose_eval=10, categorical_features=None,metrics='auc'):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric':'auc',\n",
    "        'learning_rate': 0.05, # 【consider use 0.2,0.1,0.05,0.02】\n",
    "        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n",
    "        'scale_pos_weight': 200, # because training data is extremely unbalanced\n",
    "        'num_leaves': 7,  # we should let it be smaller than 2^(max_depth), default=31\n",
    "        'max_depth': 3,  # -1 means no limit, default=-1\n",
    "        'min_data_per_leaf': 100,  # alias=min_data_per_leaf , min_data, min_child_samples, default=20\n",
    "        'max_bin': 100,  # Number of bucketed bin for feature values,default=255,set this small for more GPU speed\n",
    "        'subsample': 0.7,  # Subsample ratio of the training instance.default=1.0, alias=bagging_fraction\n",
    "        'subsample_freq': 1,  # k means will perform bagging at every k iteration, <=0 means no enable,alias=bagging_freq,default=0\n",
    "        'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.alias:feature_fraction\n",
    "        'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf),default=1e-3,Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights，【consider use to avoid overfit】\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 4, # should be equal to REAL cores:http://xgboost.readthedocs.io/en/latest/how_to/external_memory.html\n",
    "        'verbose': 0\n",
    "        \n",
    "#         'device': 'gpu',\n",
    "#         'gpu_platform_id':1\n",
    "        # gpu_use_dp, default=false,set to true to use double precision math on GPU (default using single precision)\n",
    "#         'gpu_device_id': 2 #default=-1,default value is -1, means the default device in the selected platform\n",
    "        # 'random_state':666 [LightGBM] [Warning] Unknown parameter: random_state\n",
    "        # 'feature_fraction_seed': 666,\n",
    "        # 'bagging_seed': 666, # alias=bagging_fraction_seed\n",
    "        # 'data_random_seed': 666 # random seed for data partition in parallel learning (not include feature parallel)\n",
    "    }\n",
    "    # lgb_params.update(params) # Python dict.update()\n",
    "\n",
    "    print(\"load train_df into lgb.Dataset...\")\n",
    "    # free_raw_data (bool, optional (default=True)) – If True, raw data is freed after constructing inner Dataset.\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    del dtrain\n",
    "    print(\"load valid_df into lgb.Dataset...\")\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    del dvalid\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "    \n",
    "    # Warning:basic.py:681: UserWarning: categorical_feature in param dict is overrided.\n",
    "    # https://github.com/Microsoft/LightGBM/blob/master/python-package/lightgbm/basic.py#L679\n",
    "    # https://github.com/Microsoft/LightGBM/blob/master/python-package/lightgbm/basic.py#L483\n",
    "    print('Start doing lgb.train...')\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgvalid], \n",
    "                     valid_names=['valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "    \n",
    "    del xgtrain, xgvalid\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"bst1.best_iteration: \", bst1.best_iteration)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][bst1.best_iteration-1])\n",
    "    gc.collect()\n",
    "\n",
    "    return (bst1,bst1.best_iteration)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Main function-------------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    print('Beginning...',time.asctime( time.localtime(time.time())))\n",
    "    script_start_time=time.time()\n",
    "    \n",
    "    nrows=184903891 # the first line is columns' name\n",
    "#     nchunk=150000000\n",
    "    nchunk=184903891 # entire dataset\n",
    "    val_size=62833089  # since 9th 0:00:00\n",
    "    frm=nrows-nchunk\n",
    "    \n",
    "    debug=False\n",
    "#     debug=True\n",
    "#     read=False # whether to read data from file rather than generate and save data file\n",
    "    read=True\n",
    "    if debug:\n",
    "        print('*** Debug: this is a test run for debugging purposes ***')\n",
    "        frm=0\n",
    "        nchunk=100000\n",
    "        val_size=10000\n",
    "    \n",
    "    to=frm+nchunk\n",
    "    \n",
    "#     DO(frm,to,24) # Version NO. start from 0\n",
    "    fileno = 24\n",
    "    \n",
    "    if not read:\n",
    "        dtypes = {\n",
    "                'ip'            : 'uint32',\n",
    "                'app'           : 'uint16',\n",
    "                'device'        : 'uint16',\n",
    "                'os'            : 'uint16',\n",
    "                'channel'       : 'uint16',\n",
    "                'is_attributed' : 'uint8', \n",
    "                'click_id'      : 'uint32',\n",
    "                }\n",
    "\n",
    "        print('loading train data...',frm,'to',to)\n",
    "        # usecols:Using this parameter results in much faster parsing time and lower memory usage.\n",
    "        train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "\n",
    "        print('loading test data...')\n",
    "        if debug:\n",
    "            test_df = pd.read_csv(\"../input/test_supplement.csv\", nrows=100000, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "        else:\n",
    "            test_df = pd.read_csv(\"../input/test_supplement.csv\", parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "        print('Combine test_df and train_df...')\n",
    "        len_train = len(train_df)\n",
    "        # Append rows of other to the end of this frame, returning a new object.\n",
    "        train_df=train_df.append(test_df) # Shouldn't process individually,because of lots of count,mean,var variables\n",
    "        train_df['is_attributed'].fillna(-1,inplace=True)\n",
    "        train_df['is_attributed'] = train_df['is_attributed'].astype('uint8',copy=False)\n",
    "        train_df['click_id'].fillna(-1,inplace=True)\n",
    "        train_df['click_id'] = train_df['click_id'].astype('uint32',copy=False)\n",
    "        del test_df\n",
    "        gc.collect()\n",
    "\n",
    "        print('Extracting hour and day and add 8h...')\n",
    "    #     print(type(train_df['click_time']),type(pd.to_datetime(train_df.click_time))) # Series\n",
    "    #     print(train_df['click_time'],pd.to_datetime(train_df.click_time)) # dtype: datetime64[ns]\n",
    "        # http://pandas.pydata.org/pandas-docs/stable/timeseries.html\n",
    "    #     print(train_df.click_time.head(500)) # 500 data before 6th 16:00\n",
    "        train_df.click_time = train_df.click_time + DateOffset(hours=8)\n",
    "    #     print(train_df.click_time.head(500))\n",
    "        train_df['hour'] = train_df.click_time.dt.hour.astype('uint8')\n",
    "        train_df['day'] = train_df.click_time.dt.day.astype('uint8')\n",
    "        gc.collect()\n",
    "\n",
    "        # print('the Id of train_df before function: ',id(train_df))\n",
    "        train_df = do_countuniq( train_df, ['ip'], 'channel', 'X0', 'uint8', show_max=False ); gc.collect()\n",
    "        # print('the Id of train_df after function: ',id(train_df)) # the same id with 'df' returned\n",
    "        train_df = do_countuniq( train_df, ['ip', 'day'], 'hour', 'X2', 'uint8', show_max=False ); gc.collect()\n",
    "        train_df = do_countuniq( train_df, ['ip'], 'app', 'X3', 'uint16', show_max=False ); gc.collect()\n",
    "        train_df = do_countuniq( train_df, ['ip', 'app'], 'os', 'X4', 'uint8', show_max=False ); gc.collect()\n",
    "        train_df = do_countuniq( train_df, ['ip'], 'device', 'X5', 'uint16', show_max=False ); gc.collect()\n",
    "        train_df = do_countuniq( train_df, ['app'], 'channel', 'X6','uint8', show_max=False ); gc.collect()\n",
    "        train_df = do_countuniq( train_df, ['ip', 'device', 'os'], 'app', 'X8','uint8', show_max=False ); gc.collect()\n",
    "# drop         train_df = do_cumcount( train_df, ['ip'], 'os', 'X7', show_max=False ); gc.collect()\n",
    "        train_df = do_cumcount( train_df, ['ip', 'device', 'os'], 'app', 'X1', show_max=False ); gc.collect()\n",
    "        train_df = do_countuniq( train_df, ['ip', 'device', 'os'], 'channel', 'A0', show_max=False ); gc.collect()\n",
    "        train_df = do_count( train_df, ['ip', 'app', 'channel'], 'A1', show_max=False ); gc.collect()\n",
    "        train_df = do_count( train_df, ['ip', 'device', 'os','app'], 'A2', show_max=False ); gc.collect()\n",
    "        # ip-device-hour?\n",
    "\n",
    "        train_df = do_count( train_df, ['ip', 'day', 'hour'], 'ip_tcount','uint16',show_max=False ); gc.collect()\n",
    "    #     train_df = do_count( train_df, ['ip', 'hour'], 'ip_tcount2','uint32',show_max=False ); gc.collect()\n",
    "        train_df = do_count( train_df, ['ip', 'app'], 'ip_app_count','uint32', show_max=False ); gc.collect()\n",
    "        train_df = do_count( train_df, ['ip', 'app', 'os'], 'ip_app_os_count', 'uint16', show_max=False ); gc.collect()\n",
    "        train_df = do_var( train_df, ['ip', 'day', 'channel'], 'hour', 'ip_tchan_count', show_max=False ); gc.collect()\n",
    "        train_df = do_var( train_df, ['ip', 'app', 'os'], 'hour', 'ip_app_os_var', show_max=False ); gc.collect()\n",
    "        train_df = do_var( train_df, ['ip', 'app', 'channel'], 'day', 'ip_app_channel_var_day', show_max=False ); gc.collect()\n",
    "        train_df = do_mean( train_df, ['ip', 'app', 'channel'], 'hour', 'ip_app_channel_mean_hour', show_max=False ); gc.collect()\n",
    "\n",
    "    #=====================================================================================================\n",
    "        print('doing nextClick without hash...')\n",
    "\n",
    "        train_df['click_time'] = (train_df['click_time'].astype(np.int64,copy=False) // 10 ** 9).astype(np.int32,copy=False)\n",
    "        train_df['nextClick'] = (train_df.groupby(['ip', 'app', 'device', 'os']).click_time.shift(-1) - train_df.click_time).astype(np.float32,copy=False)\n",
    "        print(train_df['nextClick'].head(50))\n",
    "        train_df.drop(['click_time','ip','day'], axis=1, inplace=True)\n",
    "        gc.collect()\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "        print(\"vars and data type: \")\n",
    "        # Warning:A value is trying to be set on a copy of a slice from a DataFrame: only test_df\n",
    "        test_df = train_df[len_train:]\n",
    "        test_df.drop(columns='is_attributed',inplace=True)\n",
    "        train_df.drop(columns='click_id',inplace=True)\n",
    "        val_df = train_df[(len_train-val_size):len_train] # Validation set\n",
    "        train_df = train_df[:(len_train-val_size)]\n",
    "        gc.collect()\n",
    "        \n",
    "        print('Generate real testset from test_supplement...')\n",
    "        test_df_real = test_df[21290876:27493809]\n",
    "        test_df_real = pd.concat([test_df_real,test_df[35678696:41791910]], copy=False)\n",
    "        test_df_real = pd.concat([test_df_real,test_df[48109937:54584259]], copy=False)\n",
    "        test_df = test_df_real\n",
    "        test_df.drop(columns='click_id',inplace=True)\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"train size: \", len(train_df))\n",
    "        print(\"valid size: \", len(val_df))\n",
    "        print(\"test size : \", len(test_df))\n",
    "        \n",
    "        print('Converting train/valid into float32...')\n",
    "        train_df = train_df.astype('float32',copy=False) # categories类型也转换为float32：精度没有问题\n",
    "        val_df = val_df.astype('float32',copy=False)\n",
    "        print(train_df.info())\n",
    "        print(val_df.info())\n",
    "        print(test_df.info())\n",
    "        gc.collect()\n",
    "        \n",
    "        print('Saving data to disk...')\n",
    "        train_df.to_pickle('./train_df_V%d.pkl'%fileno)\n",
    "        val_df.to_pickle('./val_df_V%d.pkl'%fileno)\n",
    "        test_df.to_pickle('./test_df_V%d.pkl'%fileno)\n",
    "\n",
    "        print('Preparing data time:',time.time()-script_start_time)\n",
    "        \n",
    "    else:\n",
    "        print('Reading dataset from disk file...')\n",
    "        read_fileno = 21\n",
    "        train_df = pd.read_pickle('./train_df_V%d.pkl'%read_fileno)\n",
    "        train_df.drop(columns=['X6','ip_tchan_count','ip_app_channel_var_day','B0','A1'],inplace=True)\n",
    "        gc.collect()\n",
    "        val_df = pd.read_pickle('./val_df_V%d.pkl'%read_fileno)\n",
    "        val_df.drop(columns=['X6','ip_tchan_count','ip_app_channel_var_day','B0','A1'],inplace=True)\n",
    "        gc.collect()\n",
    "#         print(train_df.info())\n",
    "#         print(val_df.info())\n",
    "#         print(test_df.info())\n",
    "        trainadd = pd.read_pickle('train_df_feature4.pkl')\n",
    "        valadd = pd.read_pickle('val_df_feature4.pkl')\n",
    "        \n",
    "        train_df['C0'] = trainadd['C0'].values\n",
    "        train_df['C1'] = trainadd['C1'].values\n",
    "        val_df['C0'] = valadd['C0'].values\n",
    "        val_df['C1'] = valadd['C1'].values\n",
    "\n",
    "        del trainadd,valadd\n",
    "        gc.collect()\n",
    "        print('Loading data time:',time.time()-script_start_time)\n",
    "    \n",
    "    predictors=[]\n",
    "    target = 'is_attributed'\n",
    "    predictors.extend(['app','device','os', 'channel', 'hour','nextClick',\n",
    "                  'ip_tcount', 'ip_app_count',\n",
    "                  'ip_app_os_count','ip_app_channel_mean_hour','ip_app_os_var',\n",
    "                  'X0', 'X1', 'X2', 'X3', 'X4', 'X5','X8',\n",
    "                     'A0','A2',\n",
    "                     'nextClick2','nextClick4','nextClick5',\n",
    "                      'nextClick6','B1','B2','B3','B4',\n",
    "                      'C0','C1'])\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "    print('predictors',predictors) # total 18+2+3+5+2=30 features\n",
    "    \n",
    "#     sub = pd.DataFrame()\n",
    "#     sub['click_id'] = test_df['click_id']\n",
    "#     print(id(sub['click_id']),id(test_df['click_id'])) # not same\n",
    "    print('Generate 9th:4,5,9,10,13,14 +8h validation set...')\n",
    "    val_df_real = val_df[(val_df.hour==12)|(val_df.hour==13)|(val_df.hour==17)|(val_df.hour==18)|(val_df.hour==21)|(val_df.hour==22)]\n",
    "    val_df.drop(val_df_real.index, axis=0, inplace=True)\n",
    "    gc.collect()\n",
    "    train_df = pd.concat([train_df,val_df], copy=False) # axis : {0/’index’, 1/’columns’}, default 0\n",
    "    \n",
    "#     print('val_df_real:')\n",
    "#     print(val_df_real.info())\n",
    "#     print('Final train_df:')\n",
    "#     print(train_df.info())\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "    \n",
    "    test_df = pd.read_pickle('./test_df_V%d.pkl'%read_fileno) # 还未转换为float32\n",
    "    test_df.drop(columns=['X6','ip_tchan_count','ip_app_channel_var_day','B0','A1'],inplace=True)\n",
    "    testadd = pd.read_pickle('test_df_feature4.pkl')\n",
    "    test_df['C0'] = testadd['C0'].values\n",
    "    test_df['C1'] = testadd['C1'].values\n",
    "    del testadd\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Training beings...\")\n",
    "    start_time = time.time()\n",
    "    (bst,best_iteration) = lgb_modelfit_nocv(\n",
    "                            train_df, \n",
    "                            val_df_real, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            early_stopping_rounds=80, \n",
    "                            verbose_eval=True, \n",
    "                            num_boost_round=3000, \n",
    "                            categorical_features=categorical)\n",
    "    del train_df\n",
    "    del val_df_real\n",
    "    gc.collect()\n",
    "    print('')\n",
    "    print('[{}]: model training time'.format(time.time() - start_time))\n",
    "\n",
    "    print(\"Predicting...\")\n",
    "    test_df[predictors] = test_df[predictors].astype('float32',copy=False)\n",
    "    test_df['is_attributed'] = bst.predict(test_df[predictors],num_iteration=best_iteration)\n",
    "    \n",
    "    if not debug:\n",
    "#         try:\n",
    "        print(\"Writing result...\")\n",
    "        sub = pd.read_csv('../input/test.csv',dtype='uint32',usecols=['click_id'])\n",
    "        test_df['click_id'] = sub\n",
    "        test_df[['click_id','is_attributed']].to_csv(b'C:\\Users\\Administrator\\Desktop\\submit_V%d.csv'%(fileno),index=False,float_format='%.9f')\n",
    "#         test_df[['click_id','is_attributed']].to_csv('./submit_V%d.csv'%(fileno),index=False,float_format='%.9f')\n",
    "        print('Save feature importances...')\n",
    "        lgb.plot_importance(bst)\n",
    "        plt.gcf().savefig(b'C:\\Users\\Administrator\\Desktop\\V%d_%d_split.png'%(fileno,nchunk),bbox_inches='tight')\n",
    "        lgb.plot_importance(bst,importance_type='gain')\n",
    "        plt.gcf().savefig(b'C:\\Users\\Administrator\\Desktop\\V%d_%d_gain.png'%(fileno,nchunk),bbox_inches='tight')\n",
    "#         except:\n",
    "#             return test_df, bst\n",
    "#     del test_df, bst\n",
    "    gc.collect()\n",
    "    \n",
    "    print('The entire running time is:',time.time()-script_start_time)\n",
    "    print(\"All finished...\",time.asctime( time.localtime(time.time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mod__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'capitalize',\n",
       " 'center',\n",
       " 'count',\n",
       " 'decode',\n",
       " 'endswith',\n",
       " 'expandtabs',\n",
       " 'find',\n",
       " 'fromhex',\n",
       " 'hex',\n",
       " 'index',\n",
       " 'isalnum',\n",
       " 'isalpha',\n",
       " 'isdigit',\n",
       " 'islower',\n",
       " 'isspace',\n",
       " 'istitle',\n",
       " 'isupper',\n",
       " 'join',\n",
       " 'ljust',\n",
       " 'lower',\n",
       " 'lstrip',\n",
       " 'maketrans',\n",
       " 'partition',\n",
       " 'replace',\n",
       " 'rfind',\n",
       " 'rindex',\n",
       " 'rjust',\n",
       " 'rpartition',\n",
       " 'rsplit',\n",
       " 'rstrip',\n",
       " 'split',\n",
       " 'splitlines',\n",
       " 'startswith',\n",
       " 'strip',\n",
       " 'swapcase',\n",
       " 'title',\n",
       " 'translate',\n",
       " 'upper',\n",
       " 'zfill']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(b'C:\\Users\\Administrator\\Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DateOffset',\n",
       " 'In',\n",
       " 'Out',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " '_sh',\n",
       " 'best_iteration',\n",
       " 'bst',\n",
       " 'categorical',\n",
       " 'debug',\n",
       " 'do_count',\n",
       " 'do_countuniq',\n",
       " 'do_cumcount',\n",
       " 'do_mean',\n",
       " 'do_var',\n",
       " 'exit',\n",
       " 'fileno',\n",
       " 'frm',\n",
       " 'gc',\n",
       " 'get_ipython',\n",
       " 'lgb',\n",
       " 'lgb_modelfit_nocv',\n",
       " 'nchunk',\n",
       " 'np',\n",
       " 'nrows',\n",
       " 'os',\n",
       " 'pd',\n",
       " 'plt',\n",
       " 'predictors',\n",
       " 'quit',\n",
       " 'read',\n",
       " 'read_fileno',\n",
       " 'script_start_time',\n",
       " 'start_time',\n",
       " 'sub',\n",
       " 'target',\n",
       " 'test_df',\n",
       " 'time',\n",
       " 'to',\n",
       " 'train_test_split',\n",
       " 'val_size']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second training\n",
    "dir() # If called without an argument, return the names in the current scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing result...\n",
      "Save feature importances...\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing result...\")\n",
    "sub = pd.read_csv('../input/test.csv',dtype='uint32',usecols=['click_id'])\n",
    "test_df['click_id'] = sub\n",
    "#     test_df[['click_id','is_attributed']].to_csv(b'C:\\Users\\Administrator\\Desktop\\submit_V%d.csv'%(fileno),index=False,float_format='%.9f')\n",
    "test_df[['click_id','is_attributed']].to_csv('./submit_V%d.csv'%(fileno),index=False,float_format='%.9f')\n",
    "print('Save feature importances...')\n",
    "lgb.plot_importance(bst)\n",
    "plt.gcf().savefig('./V%d_%d_split.png'%(fileno,nchunk),bbox_inches='tight')\n",
    "lgb.plot_importance(bst,importance_type='gain')\n",
    "plt.gcf().savefig('./V%d_%d_gain.png'%(fileno,nchunk),bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
